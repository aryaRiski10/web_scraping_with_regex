# -*- coding: utf-8 -*-
"""Scraping_Glints.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fkGXrgzDCrx38DNK12scDg3IbGTyDJpi

"""
"""Selenium libraries for infinite loop"""
import encodings
from inspect import trace
from socket import timeout
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

"""------------------------------------------------------"""

"""------------------------------------------------------"""

from tqdm import tqdm
from bs4 import BeautifulSoup
import datetime
from dateutil.relativedelta import relativedelta
from requests.adapters import HTTPAdapter

import csv
import requests
import re
import time
import pandas as pd
import db_mysql
import psutil
import tracemalloc
import os
import memory_profiler
import numpy as np

# inner psutil function
def process_memory():
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss

# decorator function
def profile(func):
    def wrapper(*args, **kwargs):
        mem_before = process_memory()
        result = func(*args, **kwargs)
        mem_after = process_memory()
        print("{}: memory before: {:,}, after: {:,}, consumed: {:,}".format(
            func.__name__,
            mem_before, mem_after, mem_after - mem_before))
 
        return result
    return wrapper

@profile
def scrapeData(tableName):
  start = time.time()
  
  global temp
  
  keywords = ["sales marketing","arts media communications","services","hotel restaurant", "education training","computer information technology","engineering","sciences"]
#   keywords = ["accounting finance","admin human resources","sales marketing","arts media communications","services","hotel restaurant",
#               "education training","computer information technology","engineering","sciences"]

  temp = pd.DataFrame(columns=['title','company','location','requirement','posted','image','link'])

  # Set driver untuk melakukan scrape dengan selenium
  def set_driver():
    PATH = Service("path/chromedriver.exe")
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--headless")
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

    driver = webdriver.Chrome(service=PATH, options=chrome_options)
    
    return driver
  
  total_pages = 3
  
  total_all = []
  total_links_all = []
  for keyword in tqdm(keywords):
    total_job_keyword = []
    total_links_keyword = []
    for page in range(1, total_pages):
        # Untuk mengambil url dengan keyword yang telah ditetapkan
        def get_url(keyword):
            keyword = keyword.replace(r' ','+')
            template = 'https://glints.com/id/opportunities/jobs/explore?keyword={}&country=ID&locationName=Indonesia&sortBy=LATEST&lastUpdated=PAST_WEEK&page={}'
            url = template.format(keyword,page)
            return url
        
        driver = set_driver()
        driver.implicitly_wait(10)
        #   driver.get(url)
        url = get_url(keyword)
        headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}
        res = requests.get(url, headers=headers) # melakukan request pada link card item untuk masuk ke dalam link tersebut 
        if res.status_code == requests.codes.ok :
            soup = BeautifulSoup(res.content,'lxml') # Fungsi beautifulsoup adalah untuk menerjemah/parsing html pada web yang dituju
        
        soupHTML = str(soup)
        # Untuk mengambil tag card item sesuai jumlah pada masing-masing halaman
        jobs = soup.find_all('div', class_=re.compile(r'.*JobcardContainer.*'))

        # Untuk mengambil link pada masing-masing card item sesuai jumlah pada masing-masing halaman
        def get_link():
            re_href = r'<a class="CompactOpportunityCardsc__CardAnchorWrapper-sc-1y4v110-18 iOjUdU job-search-results_job-card_link" href="(.*?)" target="_blank">'
            href = re.findall(re_href, soupHTML)
            
            return href

        links = []
        for link in get_link():
            link = f'https://glints.com{link}'
            links.append(link)
            
        #  Elemen tag yang diambil dan yang akan dijadikan sebagai data
        def get_data(job):
            link = links[job]
            driver.get(link)
            link_soup = BeautifulSoup(driver.page_source, 'lxml') # melakukan parsing html pada link card item tersebut
            container = link_soup.find('main',class_=re.compile('Opportunitysc__Main.*')) # mengambil tag parent yang merupakan container dari isi informasi seperti nama pekerjaan, perusahaan dan lain-lain

            containerHTML = str(container)            
            # mengambil tag nama pekerjaan
            if re.findall(r'JobOverViewTitle', containerHTML):
                re_job_name = r'<h1 class="TopFoldsc__JobOverViewTitle-sc-kklg8i-3 fFAcsE">(.*?)</h1>'
                job_name = re.findall(re_job_name, containerHTML)
            else:
                job_name = 'None'
            
            # mengambil tag nama perusahaan
            if re.findall(r'JobOverViewCompanyName', containerHTML):
                re_company_name = r'<div class="TopFoldsc__JobOverViewCompanyName-sc-kklg8i-5 eLQvRY"><a href=".*?">(.*?)</a></div>'
                company_name = re.findall(re_company_name, containerHTML)
            else:
                re_company_name = r'<span class="TopFoldsc__CompanyName-sc-kklg8i-29 hgrOYX">(.*?)</span>'
                company_name = re.findall(re_company_name, containerHTML)


            # # mengambil tag lokasi perusahaan
            if re.findall(r'JobOverViewCompanyLocation', containerHTML):
                locationHTML = container.find('div', class_=re.compile('.*CompanyLocation.*'))
                locationHTML = str(locationHTML)
                re_location = r'<a href="/id/location/.*?">(.*?)</a>'
                location = re.findall(re_location, locationHTML)
            else :
                location = 'None'

            # # mengambil tag persyaratan pekerjaan
            requirement_tag = container.find('div', class_=re.compile('.*JobDescriptionContainer.*'))
            if requirement_tag is not None:
                for requirement in requirement_tag.find_all('div', class_=re.compile('.*DescriptionContainer.*')):
                    requirement = requirement.text
            else:
                requirement = 'None'
            
            # # mengambil tag waktu dari pekerjaan itu di posting
            if re.findall(r'JobOverviewTime', containerHTML):
                re_posted = r'<span class="TopFoldsc__UpdatedAt-sc-kklg8i-12 bYndtI" data-recent=".*?">(.*?)</span>'
                posted = re.findall(re_posted, containerHTML)
            elif re.findall(r'OpportunityMeta', soupHTML):
                re_posted = r'<span data-recent=".*?" class="CompactOpportunityCardsc__UpdatedAtMessage-sc-1y4v110-17 jgBEKn">(.*?)</span>'
                posted = re.findall(re_posted, soupHTML)
            else:
                posted = 'None'
                
            # # mengambil tag gambar perusahaan   
            if re.findall(r'CompanyLogo', containerHTML):
                imageHTML = container.find('div', class_=re.compile('CompanyLogo.*'))
                imageHTML = str(imageHTML)
                re_image = r'src="(.*?)"'
                image = re.findall(re_image, imageHTML)
            else:
                image = 'None'
                        
            data = [job_name,company_name,location,requirement,posted,image,link]
            return data

        for cjob in jobs:
            total_job_keyword.append(cjob)     
        for l in links:
            total_links_keyword .append(l)
            
        records=[]
        if len(jobs) != 0:
            for job in range(len(jobs)):
                try:
                    record = get_data(job)
                    records.append(record)
                except:
                    pass
        else:
            break

        df = pd.DataFrame(records, columns=['title','company','location','requirement','posted','image','link'])
        temp = pd.concat([temp,df])


        temp.reset_index(inplace=True, drop=True)
        data_df = temp[['title','company','location','requirement','posted','image','link']].copy()

        data_df = data_df.dropna(how='any')
        data_df = data_df.reset_index(drop=True)
        data_df['title'] = data_df['title'].str.get(0)
        data_df['company'] = data_df['company'].str.get(0)
        data_df['location'] = data_df['location'].str.get(0)
        data_df['posted'] = data_df['posted'].str.get(0)
        data_df['image'] = data_df['image'].str.get(0)

    for j in total_job_keyword:
        total_all.append(j)
    for l in total_links_keyword:
        total_links_all.append(l)
            
    date_posted = []
    for i in data_df['posted']:
        if re.findall(r'[0-9]+ (hari|day)', str(i)):
            toSingleString = ''.join(re.findall(r'[0-9]+',i))           
            toInt = (datetime.datetime.today() - datetime.timedelta(int(toSingleString))).strftime('%Y-%m-%d')
            date_posted.append(toInt)       
        elif re.findall(r'[0-9]+ (bulan|month)', str(i)):
            toSingleString = ''.join(re.findall(r'[0-9]+',i))
            toInt = (datetime.datetime.today() - relativedelta(months=int(toSingleString))).strftime('%Y-%m-%d')
            date_posted.append(toInt)
        else:
            date_posted.append(datetime.datetime.today().strftime('%Y-%m-%d'))
    
    data_df['date_posted'] = date_posted

    data_df_for_csv = data_df.copy()

    data_df_for_csv = data_df_for_csv.replace(r'\n','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'\t','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'\r','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'\s\s\s','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'Bisa kerja remote','', regex=True)
    data_df_for_csv['location'] = [re.sub(r'(\w)([A-Z])', r'\1, \2', ele) for ele in data_df_for_csv['location']]
    data_df_for_csv['requirement'] = [re.sub(r'([a-zA-Z])([A-Z][a-z]+)', r'\1. \2', ele) for ele in data_df_for_csv['requirement']]
   
  datetime_scrape = datetime.datetime.today().strftime("%Y-%m-%d")
  data_df_for_csv.to_excel(f'data_csv\jobs_data_glints({datetime_scrape}).xlsx',index=False)
  
  data_df.fillna('None')
  db_mysql.insertData(data_df, tableName)
  db_mysql.removeDuplicate(tableName)

  end = time.time()

  print('Scrape is finished..')
  print('Waktu scraping data kalibrr: ', end-start)
  print('total : ',len(data_df.index))
  print('total seluruh job :', len(total_all))
  print('total seluruh link:', len(total_links_all))
  print('total yang didapat :', len(data_df.index))
  return data_df

scrapeData('public.scrape_items')